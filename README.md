# OpenBreweryDB Lakehouse Pipeline  
**(Databricks | Medallion Architecture | DQ | Orchestration | Observability | Deployment)**

This project implements a production-grade **Data Engineering pipeline** using:

- **Databricks Serverless (Free Edition)**
- **Unity Catalog + Volumes**
- **Delta Lake**
- **Medallion Architecture (Bronze â†’ Silver â†’ Gold)**
- **Incremental & Full Processing**
- **Data Quality Framework**
- **Orchestration (Workflows / Airflow-ready)**
- **Observability + Alerts**
- **Docker/Kubernetes Deployment Design**

The pipeline ingests brewery data from:

ğŸ‘‰ https://www.openbrewerydb.org/

It demonstrates end-to-end capabilities expected in enterprise-grade data platforms, including ingestion, governance, validation, monitoring, orchestration, and deployment.

---

# Architecture

## Design Goals

- Implement a Lakehouse using **Delta Lake** for reliability and ACID guarantees  
- Use **Medallion Architecture** for layered data refinement  
- Store raw data in **Volumes** (Bronze)  
- Store curated/aggregated data in **Unity Catalog managed tables** (Silver/Gold)  
- Support **incremental** and **full-refresh** transforms  
- Integrate a **robust Data Quality framework**  
- Provide **orchestration + observability**  
- Be deployable in **Docker/Kubernetes**

---

## ğŸ§± Architecture Blueprint

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   OpenBreweryDB API     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚ REST JSON
                                   â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚          BRONZE                â”‚
                  â”‚ Delta Lake on Volumes          â”‚
                  â”‚ Raw snapshots (_ingestion_date)â”‚
                  â”‚ Append-only ingestion          â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                   Read from Volume path
                                 â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚           SILVER          â”‚
                   â”‚ Unity Catalog Managed     â”‚
                   â”‚ Clean, standardized       â”‚
                   â”‚ FULL Weekly + Incremental |
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                         MERGE INTO Silver
                                  â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚             GOLD             â”‚
                   â”‚ Business Aggregations        â”‚
                   â”‚ Breweries per city/state/typeâ”‚
                   â”‚ FULL Weekly + Incremental    â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Storage Layout

| Layer | Storage | Description |
|-------|---------|-------------|
| Bronze | Volume Delta Lake | Raw data snapshots |
| Silver | UC Managed | Clean business entities |
| Gold | UC Managed | Aggregated KPIs |

---

# ğŸ”„ Orchestration Strategy

The pipeline is orchestrated using **Databricks Workflows**, supporting:

- Task dependencies  
- Automated retries  
- Failure handling  
- Notifications  
- Serverless execution  

The solution provides **two pipelines**:  
âœ” Daily (incremental)  
âœ” Weekly (full + DQ)

---

## ğŸ—“ Daily Workflow (Incremental)

```
Bronze Ingest â†’ Silver Incremental â†’ Gold Incremental
```

Purpose:

- Fast, lightweight processing  
- Updates the curated and aggregated layers  
- No Data Quality (DQ) check to reduce operational overhead

---

## ğŸ—“ Weekly Workflow (Full + DQ)

```
Bronze Ingest (weekly)
       â†“
Silver FULL
       â†“
Gold FULL
       â†“
DQ Runner (Bronze + Silver + Gold)
       â†“
Bronze Cleanup
```

Purpose:

- Rebuild Silver and Gold entirely  
- Run full platform-wide Data Quality checks  
- Clean Bronze snapshots with retention window  

---

## ğŸ› Alternative Orchestration (Airflow, Mage, Luigi)

This project can also be orchestrated using external tools.

### Example Airflow DAG

```python
with DAG("brewery_weekly"):

    bronze = PythonOperator(...)
    silver_full = PythonOperator(...)
    gold_full = PythonOperator(...)
    dq = PythonOperator(...)
    cleanup = PythonOperator(...)

    bronze >> silver_full >> gold_full >> dq >> cleanup
```

This demonstrates understanding of:

- Task retries  
- SLA monitoring  
- Failure callbacks  
- Distributed scheduling  

---

# ğŸ§ª Data Quality Framework (DQ)

This solution implements Data Quality across all layers using multiple dimensions:

---

## ğŸ“ DQ Dimensions Implemented

| Dimension | Bronze | Silver | Gold |
|-----------|--------|--------|------|
| Freshness | âœ” | âœ” | â€” |
| Completeness | âœ” | âœ” | âœ” |
| Schema Drift | âœ” | â€” | â€” |
| Uniqueness | âœ” | âœ” | â€” |
| Validity | âœ” | âœ” | âœ” |
| Consistency | â€” | âœ” | âœ” |

---

## ğŸŸ« Bronze DQ

Checks include:

- Schema drift detection  
- Required fields (`id`, `name`) not null  
- Uniqueness per snapshot  
- Freshness validation  
- Volume anomaly detection (empty ingestion)  

---

## ğŸŸ¦ Silver DQ  
*(Updated with `try_cast` to avoid failures on malformed latitude/longitude)*

Silver validations include:

- Unique ID  
- Completeness of key business fields  
- Valid brewery type values  
- Proper formatting (country uppercase, city standardized)  
- Valid latitude/longitude range using:

```sql
try_cast(latitude AS double)
```

This avoids pipeline failures due to API inconsistencies.

---

## ğŸŸ¡ Gold DQ

Checks include:

- No negative aggregates  
- No null grouping keys  
- Country format consistency  
- Structural schema validation  

---

## ğŸ§¾ DQ Runner (Weekly)

The *dq_runner.py* orchestrates all quality checks and stores results into:

```
<catalog>.quality.dq_audit
```

Audit fields:

| layer | check_name | status | timestamp | details |

This enables governance, alerting, trend analysis, and failure diagnostics.

---

# ğŸ‘€ Observability & Alerts

Enterprise observability is achieved using:

---

## 1ï¸âƒ£ DQ Audit Table

Centralized quality tracking.

Example query:

```sql
SELECT *
FROM brewery_prod.quality.dq_audit
WHERE status = false
ORDER BY timestamp DESC;
```

---

## 2ï¸âƒ£ Workflow Monitoring

Databricks natively provides:

- Task execution logs  
- Runtime graphs  
- Failure traces  
- Retry history  
- Audit logs  

---

## 3ï¸âƒ£ Alerts (Email, Slack, Teams)

Databricks Workflow notifications:

- On failure  
- On timeout  
- On retry  

Example Slack webhook:

```json
{ "text": "ğŸš¨ Brewery Pipeline Failure: check dq_audit table." }
```

---

# ğŸš€ Deployment (Docker / Kubernetes)

Even though Databricks handles infrastructure, the project includes a modular deployment design for environments where portability is required.

---

## ğŸ³ Docker

Example Dockerfile:

```dockerfile
FROM python:3.10-slim

COPY . /app
WORKDIR /app

RUN pip install -r requirements.txt

CMD ["python", "bronze/bronze_ingest_delta.py"]
```

---

## â˜¸ Kubernetes (CronJobs)

Daily:

```yaml
schedule: "0 2 * * *"
command: ["python", "silver/silver_transform_incremental.py"]
```

Weekly:

```yaml
schedule: "0 3 * * SUN"
command: ["python", "dq/dq_runner.py"]
```

Provides:

- Autoscaling  
- Fault tolerance  
- Centralized logging (ELK/Grafana)  
- Infrastructure as code  

---

# ğŸ¯ Final Result

This project delivers a full-featured **Lakehouse Data Platform**:

### âœ” Scalable architecture  
### âœ” Robust ingestion with schema normalization  
### âœ” Comprehensive DQ checks across all Medallion layers  
### âœ” Incremental + full-refresh pipelines  
### âœ” Orchestration with alerts & observability  
### âœ” Deployment-ready design for Docker/K8s  
### âœ” Governed data in Unity Catalog  
### âœ” Automated cleanup and retention policies  

Perfect for showcasing **Data Engineering, Data Quality, Architecture, and DevOps** capabilities in a modern data platform.
